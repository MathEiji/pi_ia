{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d91093c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Imports para executar o treinamento, em casos de duvidas fico a disposicao pois foi um caos ajustar todas as dependencias\n",
    "\n",
    "# !pip install tensorflow==2.12.0 gym==0.21.0 keras-rl2=1.0.5 gym[atari] gymnasium[accept-rom-license] gymnasium[atari]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89c3f098",
   "metadata": {},
   "source": [
    "# Testando os imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c76e5ad0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "142e6489",
   "metadata": {},
   "outputs": [],
   "source": [
    "from colorama import just_fix_windows_console, init, Back, Fore\n",
    "\n",
    "init(autoreset=True)\n",
    "\n",
    "# use Colorama to make Termcolor work on Windows too\n",
    "just_fix_windows_console()\n",
    "environment_name = \"ALE/SpaceInvaders-v5\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c8fc613c",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(environment_name)\n",
    "height, width, channels = env.observation_space.shape\n",
    "actions = env.action_space.n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "19b03d9b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['NOOP', 'FIRE', 'RIGHT', 'LEFT', 'RIGHTFIRE', 'LEFTFIRE']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.unwrapped.get_action_meanings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1116766",
   "metadata": {},
   "outputs": [],
   "source": [
    "episodes = 5\n",
    "for episode in range(1, episodes+1):\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    score = 0 \n",
    "    \n",
    "    while not done:\n",
    "        env.render()\n",
    "        action = random.choice([0,1,2,3,4,5])\n",
    "        n_state, reward, done, info = env.step(action)\n",
    "        score+=reward\n",
    "    print('Episode:{} Score:{}'.format(episode, score))\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22f0a323",
   "metadata": {},
   "source": [
    "# Criando e treinando o modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e1b50422",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Flatten, Convolution2D\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import tensorflow as tf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "82eef89a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(height, width, channels, actions):\n",
    "    model = Sequential()\n",
    "    model.add(Convolution2D(32, (8,8), strides=(4,4), activation='relu', input_shape=(3,height, width, channels)))\n",
    "    model.add(Convolution2D(64, (4,4), strides=(2,2), activation='relu'))\n",
    "    model.add(Convolution2D(64, (3,3), activation='relu'))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(512, activation='relu'))\n",
    "    model.add(Dense(256, activation='relu'))\n",
    "    model.add(Dense(actions, activation='linear'))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a110628b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_model(height, width, channels, actions)\n",
    "\n",
    "del model\n",
    "\n",
    "model = build_model(height, width, channels, actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a4ade3e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rl.agents import DQNAgent\n",
    "from rl.memory import SequentialMemory\n",
    "from rl.policy import LinearAnnealedPolicy, EpsGreedyQPolicy\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "af6d39cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_agent(model, actions):\n",
    "    policy = LinearAnnealedPolicy(EpsGreedyQPolicy(), attr='eps', value_max=1., value_min=.1, value_test=.2, nb_steps=10000)\n",
    "    memory = SequentialMemory(limit=10000, window_length=3)\n",
    "    dqn = DQNAgent(model=model, memory=memory, policy=policy,\n",
    "                  enable_dueling_network=True, dueling_type='avg', \n",
    "                   nb_actions=actions, nb_steps_warmup=1000\n",
    "                  )\n",
    "    return dqn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9034c717",
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = tf.keras.optimizers.legacy.Adam(learning_rate=0.0001)\n",
    "\n",
    "dqn = build_agent(model, actions)\n",
    "dqn.compile(optimizer=opt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9889d8e9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for 100000 steps ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Eiji\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\engine\\training_v1.py:2359: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   296/100000: episode: 1, duration: 6.131s, episode steps: 296, steps per second:  48, episode reward: 50.000, mean reward:  0.169 [ 0.000, 20.000], mean action: 2.304 [0.000, 5.000],  loss: --, mean_q: --, mean_eps: --\n",
      "   868/100000: episode: 2, duration: 11.455s, episode steps: 572, steps per second:  50, episode reward: 135.000, mean reward:  0.236 [ 0.000, 30.000], mean action: 2.302 [0.000, 5.000],  loss: --, mean_q: --, mean_eps: --\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Eiji\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\engine\\training_v1.py:2359: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  1424/100000: episode: 3, duration: 321.734s, episode steps: 556, steps per second:   2, episode reward: 170.000, mean reward:  0.306 [ 0.000, 30.000], mean action: 2.674 [0.000, 5.000],  loss: 8.936710, mean_q: 7.363911, mean_eps: 0.890920\n",
      "  2138/100000: episode: 4, duration: 551.843s, episode steps: 714, steps per second:   1, episode reward: 225.000, mean reward:  0.315 [ 0.000, 30.000], mean action: 2.419 [0.000, 5.000],  loss: 1.546925, mean_q: 7.022682, mean_eps: 0.839755\n",
      "  3126/100000: episode: 5, duration: 774.366s, episode steps: 988, steps per second:   1, episode reward: 290.000, mean reward:  0.294 [ 0.000, 30.000], mean action: 2.503 [0.000, 5.000],  loss: 0.914421, mean_q: 6.810249, mean_eps: 0.763165\n",
      "  3532/100000: episode: 6, duration: 382.236s, episode steps: 406, steps per second:   1, episode reward: 155.000, mean reward:  0.382 [ 0.000, 30.000], mean action: 2.448 [0.000, 5.000],  loss: 0.662070, mean_q: 6.732343, mean_eps: 0.700435\n",
      "  3971/100000: episode: 7, duration: 429.336s, episode steps: 439, steps per second:   1, episode reward: 120.000, mean reward:  0.273 [ 0.000, 30.000], mean action: 2.508 [0.000, 5.000],  loss: 0.691764, mean_q: 6.754043, mean_eps: 0.662410\n",
      "  4389/100000: episode: 8, duration: 437.659s, episode steps: 418, steps per second:   1, episode reward: 110.000, mean reward:  0.263 [ 0.000, 30.000], mean action: 2.203 [0.000, 5.000],  loss: 0.587952, mean_q: 6.646169, mean_eps: 0.623845\n",
      "  4799/100000: episode: 9, duration: 456.769s, episode steps: 410, steps per second:   1, episode reward: 20.000, mean reward:  0.049 [ 0.000, 10.000], mean action: 2.085 [0.000, 5.000],  loss: 0.627588, mean_q: 6.775179, mean_eps: 0.586585\n",
      "  5336/100000: episode: 10, duration: 624.547s, episode steps: 537, steps per second:   1, episode reward: 130.000, mean reward:  0.242 [ 0.000, 25.000], mean action: 2.328 [0.000, 5.000],  loss: 0.307735, mean_q: 6.435689, mean_eps: 0.543970\n",
      "  5627/100000: episode: 11, duration: 357.102s, episode steps: 291, steps per second:   1, episode reward: 50.000, mean reward:  0.172 [ 0.000, 20.000], mean action: 2.667 [0.000, 5.000],  loss: 0.270208, mean_q: 6.362313, mean_eps: 0.506710\n",
      "  6238/100000: episode: 12, duration: 794.249s, episode steps: 611, steps per second:   1, episode reward: 210.000, mean reward:  0.344 [ 0.000, 30.000], mean action: 2.399 [0.000, 5.000],  loss: 0.389336, mean_q: 6.332670, mean_eps: 0.466120\n",
      "  6675/100000: episode: 13, duration: 586.689s, episode steps: 437, steps per second:   1, episode reward: 45.000, mean reward:  0.103 [ 0.000, 15.000], mean action: 2.172 [0.000, 5.000],  loss: 0.463414, mean_q: 6.230685, mean_eps: 0.418960\n",
      "  7227/100000: episode: 14, duration: 773.521s, episode steps: 552, steps per second:   1, episode reward: 135.000, mean reward:  0.245 [ 0.000, 30.000], mean action: 2.185 [0.000, 5.000],  loss: 0.429144, mean_q: 6.204128, mean_eps: 0.374455\n",
      "  7698/100000: episode: 15, duration: 691.641s, episode steps: 471, steps per second:   1, episode reward: 105.000, mean reward:  0.223 [ 0.000, 30.000], mean action: 2.242 [0.000, 5.000],  loss: 0.468692, mean_q: 6.096046, mean_eps: 0.328420\n",
      "  8937/100000: episode: 16, duration: 2033.467s, episode steps: 1239, steps per second:   1, episode reward: 435.000, mean reward:  0.351 [ 0.000, 30.000], mean action: 2.487 [0.000, 5.000],  loss: 0.447607, mean_q: 6.055732, mean_eps: 0.251470\n",
      "  9420/100000: episode: 17, duration: 908.219s, episode steps: 483, steps per second:   1, episode reward: 120.000, mean reward:  0.248 [ 0.000, 30.000], mean action: 2.747 [0.000, 5.000],  loss: 0.535583, mean_q: 6.006715, mean_eps: 0.173980\n",
      "  9860/100000: episode: 18, duration: 837.431s, episode steps: 440, steps per second:   1, episode reward: 30.000, mean reward:  0.068 [ 0.000, 20.000], mean action: 2.052 [0.000, 5.000],  loss: 0.347181, mean_q: 5.898077, mean_eps: 0.132445\n",
      " 10291/100000: episode: 19, duration: 849.487s, episode steps: 431, steps per second:   1, episode reward: 95.000, mean reward:  0.220 [ 0.000, 25.000], mean action: 2.415 [0.000, 5.000],  loss: 1.609551, mean_q: 6.617024, mean_eps: 0.102061\n",
      " 10573/100000: episode: 20, duration: 557.099s, episode steps: 282, steps per second:   1, episode reward: 20.000, mean reward:  0.071 [ 0.000, 10.000], mean action: 2.500 [0.000, 5.000],  loss: 1.271835, mean_q: 6.867095, mean_eps: 0.100000\n",
      " 10836/100000: episode: 21, duration: 522.116s, episode steps: 263, steps per second:   1, episode reward: 45.000, mean reward:  0.171 [ 0.000, 15.000], mean action: 1.890 [0.000, 5.000],  loss: 0.839270, mean_q: 6.694738, mean_eps: 0.100000\n",
      " 11408/100000: episode: 22, duration: 1120.018s, episode steps: 572, steps per second:   1, episode reward: 45.000, mean reward:  0.079 [ 0.000, 15.000], mean action: 1.740 [0.000, 5.000],  loss: 0.543114, mean_q: 6.655788, mean_eps: 0.100000\n",
      " 11753/100000: episode: 23, duration: 665.786s, episode steps: 345, steps per second:   1, episode reward: 55.000, mean reward:  0.159 [ 0.000, 25.000], mean action: 2.272 [0.000, 5.000],  loss: 0.393534, mean_q: 6.562902, mean_eps: 0.100000\n",
      " 12275/100000: episode: 24, duration: 990.908s, episode steps: 522, steps per second:   1, episode reward: 95.000, mean reward:  0.182 [ 0.000, 20.000], mean action: 1.923 [0.000, 5.000],  loss: 0.391370, mean_q: 6.465209, mean_eps: 0.100000\n",
      " 12527/100000: episode: 25, duration: 472.745s, episode steps: 252, steps per second:   1, episode reward: 30.000, mean reward:  0.119 [ 0.000, 10.000], mean action: 2.310 [0.000, 5.000],  loss: 0.243154, mean_q: 6.475647, mean_eps: 0.100000\n",
      " 12845/100000: episode: 26, duration: 597.158s, episode steps: 318, steps per second:   1, episode reward: 45.000, mean reward:  0.142 [ 0.000, 30.000], mean action: 2.022 [0.000, 5.000],  loss: 0.251566, mean_q: 6.472791, mean_eps: 0.100000\n",
      " 13499/100000: episode: 27, duration: 1226.030s, episode steps: 654, steps per second:   1, episode reward: 135.000, mean reward:  0.206 [ 0.000, 30.000], mean action: 2.125 [0.000, 5.000],  loss: 0.330497, mean_q: 6.376399, mean_eps: 0.100000\n",
      " 13963/100000: episode: 28, duration: 871.090s, episode steps: 464, steps per second:   1, episode reward: 105.000, mean reward:  0.226 [ 0.000, 30.000], mean action: 2.267 [0.000, 5.000],  loss: 0.286426, mean_q: 6.285936, mean_eps: 0.100000\n",
      " 14728/100000: episode: 29, duration: 1435.210s, episode steps: 765, steps per second:   1, episode reward: 290.000, mean reward:  0.379 [ 0.000, 30.000], mean action: 2.685 [0.000, 5.000],  loss: 0.389201, mean_q: 6.276775, mean_eps: 0.100000\n",
      " 15123/100000: episode: 30, duration: 740.432s, episode steps: 395, steps per second:   1, episode reward: 30.000, mean reward:  0.076 [ 0.000, 25.000], mean action: 1.954 [0.000, 5.000],  loss: 0.461126, mean_q: 6.271198, mean_eps: 0.100000\n",
      " 15582/100000: episode: 31, duration: 860.423s, episode steps: 459, steps per second:   1, episode reward: 135.000, mean reward:  0.294 [ 0.000, 30.000], mean action: 2.617 [0.000, 5.000],  loss: 0.324105, mean_q: 6.253579, mean_eps: 0.100000\n",
      " 15876/100000: episode: 32, duration: 551.339s, episode steps: 294, steps per second:   1, episode reward: 45.000, mean reward:  0.153 [ 0.000, 15.000], mean action: 2.687 [0.000, 5.000],  loss: 0.359288, mean_q: 6.249175, mean_eps: 0.100000\n",
      " 16444/100000: episode: 33, duration: 1060.890s, episode steps: 568, steps per second:   1, episode reward: 130.000, mean reward:  0.229 [ 0.000, 30.000], mean action: 2.783 [0.000, 5.000],  loss: 0.265625, mean_q: 6.273435, mean_eps: 0.100000\n",
      " 16778/100000: episode: 34, duration: 621.998s, episode steps: 334, steps per second:   1, episode reward: 90.000, mean reward:  0.269 [ 0.000, 30.000], mean action: 2.467 [0.000, 5.000],  loss: 0.314582, mean_q: 6.219960, mean_eps: 0.100000\n",
      " 17070/100000: episode: 35, duration: 543.344s, episode steps: 292, steps per second:   1, episode reward: 45.000, mean reward:  0.154 [ 0.000, 15.000], mean action: 2.671 [0.000, 5.000],  loss: 0.299262, mean_q: 6.305743, mean_eps: 0.100000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 17673/100000: episode: 36, duration: 1116.639s, episode steps: 603, steps per second:   1, episode reward: 140.000, mean reward:  0.232 [ 0.000, 25.000], mean action: 2.670 [0.000, 5.000],  loss: 0.212034, mean_q: 6.174518, mean_eps: 0.100000\n",
      " 18500/100000: episode: 37, duration: 1527.724s, episode steps: 827, steps per second:   1, episode reward: 155.000, mean reward:  0.187 [ 0.000, 30.000], mean action: 2.064 [0.000, 5.000],  loss: 0.271975, mean_q: 6.156345, mean_eps: 0.100000\n",
      " 18993/100000: episode: 38, duration: 911.348s, episode steps: 493, steps per second:   1, episode reward: 100.000, mean reward:  0.203 [ 0.000, 30.000], mean action: 1.935 [0.000, 5.000],  loss: 0.180735, mean_q: 6.173980, mean_eps: 0.100000\n",
      " 19348/100000: episode: 39, duration: 657.619s, episode steps: 355, steps per second:   1, episode reward: 40.000, mean reward:  0.113 [ 0.000, 15.000], mean action: 2.865 [0.000, 5.000],  loss: 0.394117, mean_q: 6.186566, mean_eps: 0.100000\n",
      " 19822/100000: episode: 40, duration: 878.822s, episode steps: 474, steps per second:   1, episode reward: 75.000, mean reward:  0.158 [ 0.000, 25.000], mean action: 2.521 [0.000, 5.000],  loss: 0.184241, mean_q: 6.246397, mean_eps: 0.100000\n",
      " 20237/100000: episode: 41, duration: 766.943s, episode steps: 415, steps per second:   1, episode reward: 30.000, mean reward:  0.072 [ 0.000, 25.000], mean action: 2.398 [0.000, 5.000],  loss: 1.056573, mean_q: 6.538718, mean_eps: 0.100000\n",
      " 20688/100000: episode: 42, duration: 832.280s, episode steps: 451, steps per second:   1, episode reward: 105.000, mean reward:  0.233 [ 0.000, 25.000], mean action: 1.736 [0.000, 5.000],  loss: 0.915423, mean_q: 6.791426, mean_eps: 0.100000\n",
      " 21040/100000: episode: 43, duration: 649.220s, episode steps: 352, steps per second:   1, episode reward: 65.000, mean reward:  0.185 [ 0.000, 20.000], mean action: 1.545 [0.000, 5.000],  loss: 0.615082, mean_q: 6.838377, mean_eps: 0.100000\n",
      " 21344/100000: episode: 44, duration: 559.995s, episode steps: 304, steps per second:   1, episode reward: 30.000, mean reward:  0.099 [ 0.000, 15.000], mean action: 2.378 [0.000, 5.000],  loss: 0.476638, mean_q: 6.791354, mean_eps: 0.100000\n",
      " 22065/100000: episode: 45, duration: 1323.029s, episode steps: 721, steps per second:   1, episode reward: 440.000, mean reward:  0.610 [ 0.000, 200.000], mean action: 1.871 [0.000, 5.000],  loss: 2.048374, mean_q: 6.859615, mean_eps: 0.100000\n",
      " 22524/100000: episode: 46, duration: 842.167s, episode steps: 459, steps per second:   1, episode reward: 120.000, mean reward:  0.261 [ 0.000, 30.000], mean action: 1.699 [0.000, 5.000],  loss: 2.750238, mean_q: 6.869143, mean_eps: 0.100000\n",
      " 23140/100000: episode: 47, duration: 1130.496s, episode steps: 616, steps per second:   1, episode reward: 100.000, mean reward:  0.162 [ 0.000, 20.000], mean action: 2.214 [0.000, 5.000],  loss: 1.319929, mean_q: 6.935097, mean_eps: 0.100000\n",
      " 23428/100000: episode: 48, duration: 527.274s, episode steps: 288, steps per second:   1, episode reward: 40.000, mean reward:  0.139 [ 0.000, 15.000], mean action: 1.729 [0.000, 5.000],  loss: 0.921887, mean_q: 7.041261, mean_eps: 0.100000\n",
      " 23908/100000: episode: 49, duration: 876.229s, episode steps: 480, steps per second:   1, episode reward: 110.000, mean reward:  0.229 [ 0.000, 30.000], mean action: 2.229 [0.000, 5.000],  loss: 0.422431, mean_q: 6.898168, mean_eps: 0.100000\n",
      " 24773/100000: episode: 50, duration: 1551.337s, episode steps: 865, steps per second:   1, episode reward: 305.000, mean reward:  0.353 [ 0.000, 30.000], mean action: 1.741 [0.000, 5.000],  loss: 0.438179, mean_q: 6.806114, mean_eps: 0.100000\n",
      " 25050/100000: episode: 51, duration: 493.973s, episode steps: 277, steps per second:   1, episode reward: 60.000, mean reward:  0.217 [ 0.000, 30.000], mean action: 2.296 [0.000, 5.000],  loss: 0.884535, mean_q: 6.706505, mean_eps: 0.100000\n",
      " 25359/100000: episode: 52, duration: 551.048s, episode steps: 309, steps per second:   1, episode reward: 80.000, mean reward:  0.259 [ 0.000, 25.000], mean action: 2.362 [0.000, 5.000],  loss: 0.886228, mean_q: 6.819363, mean_eps: 0.100000\n",
      " 25974/100000: episode: 53, duration: 1096.969s, episode steps: 615, steps per second:   1, episode reward: 380.000, mean reward:  0.618 [ 0.000, 200.000], mean action: 1.948 [0.000, 5.000],  loss: 0.374369, mean_q: 6.789512, mean_eps: 0.100000\n",
      " 26435/100000: episode: 54, duration: 819.900s, episode steps: 461, steps per second:   1, episode reward: 120.000, mean reward:  0.260 [ 0.000, 25.000], mean action: 2.228 [0.000, 5.000],  loss: 2.965746, mean_q: 6.863699, mean_eps: 0.100000\n",
      " 27282/100000: episode: 55, duration: 1499.500s, episode steps: 847, steps per second:   1, episode reward: 385.000, mean reward:  0.455 [ 0.000, 200.000], mean action: 1.800 [0.000, 5.000],  loss: 1.122901, mean_q: 6.764202, mean_eps: 0.100000\n",
      " 27569/100000: episode: 56, duration: 508.323s, episode steps: 287, steps per second:   1, episode reward: 35.000, mean reward:  0.122 [ 0.000, 15.000], mean action: 2.941 [0.000, 5.000],  loss: 4.185071, mean_q: 6.716130, mean_eps: 0.100000\n",
      " 28065/100000: episode: 57, duration: 872.869s, episode steps: 496, steps per second:   1, episode reward: 80.000, mean reward:  0.161 [ 0.000, 20.000], mean action: 1.839 [0.000, 5.000],  loss: 2.499880, mean_q: 6.763791, mean_eps: 0.100000\n",
      " 28550/100000: episode: 58, duration: 850.931s, episode steps: 485, steps per second:   1, episode reward: 120.000, mean reward:  0.247 [ 0.000, 30.000], mean action: 1.961 [0.000, 5.000],  loss: 3.527884, mean_q: 6.999423, mean_eps: 0.100000\n",
      " 29388/100000: episode: 59, duration: 1463.148s, episode steps: 838, steps per second:   1, episode reward: 295.000, mean reward:  0.352 [ 0.000, 30.000], mean action: 1.739 [0.000, 5.000],  loss: 1.413877, mean_q: 6.855017, mean_eps: 0.100000\n",
      " 29873/100000: episode: 60, duration: 846.167s, episode steps: 485, steps per second:   1, episode reward: 135.000, mean reward:  0.278 [ 0.000, 30.000], mean action: 2.285 [0.000, 5.000],  loss: 0.807682, mean_q: 6.736859, mean_eps: 0.100000\n",
      " 30272/100000: episode: 61, duration: 698.522s, episode steps: 399, steps per second:   1, episode reward: 40.000, mean reward:  0.100 [ 0.000, 15.000], mean action: 1.767 [0.000, 5.000],  loss: 3.038004, mean_q: 7.163832, mean_eps: 0.100000\n",
      " 30832/100000: episode: 62, duration: 976.898s, episode steps: 560, steps per second:   1, episode reward: 140.000, mean reward:  0.250 [ 0.000, 30.000], mean action: 2.314 [0.000, 5.000],  loss: 6.563389, mean_q: 7.470001, mean_eps: 0.100000\n",
      " 31231/100000: episode: 63, duration: 695.286s, episode steps: 399, steps per second:   1, episode reward: 125.000, mean reward:  0.313 [ 0.000, 20.000], mean action: 2.424 [0.000, 5.000],  loss: 4.702782, mean_q: 7.532043, mean_eps: 0.100000\n",
      " 31833/100000: episode: 64, duration: 1045.997s, episode steps: 602, steps per second:   1, episode reward: 15.000, mean reward:  0.025 [ 0.000,  5.000], mean action: 1.696 [0.000, 5.000],  loss: 2.594757, mean_q: 7.360428, mean_eps: 0.100000\n",
      " 32524/100000: episode: 65, duration: 1198.152s, episode steps: 691, steps per second:   1, episode reward: 195.000, mean reward:  0.282 [ 0.000, 30.000], mean action: 2.191 [0.000, 5.000],  loss: 0.769100, mean_q: 7.294663, mean_eps: 0.100000\n",
      " 32914/100000: episode: 66, duration: 674.169s, episode steps: 390, steps per second:   1, episode reward: 80.000, mean reward:  0.205 [ 0.000, 20.000], mean action: 2.000 [0.000, 5.000],  loss: 1.058702, mean_q: 7.217301, mean_eps: 0.100000\n",
      " 33978/100000: episode: 67, duration: 1829.650s, episode steps: 1064, steps per second:   1, episode reward: 560.000, mean reward:  0.526 [ 0.000, 200.000], mean action: 1.955 [0.000, 5.000],  loss: 1.195903, mean_q: 7.083689, mean_eps: 0.100000\n",
      " 34558/100000: episode: 68, duration: 997.476s, episode steps: 580, steps per second:   1, episode reward: 135.000, mean reward:  0.233 [ 0.000, 30.000], mean action: 2.495 [0.000, 5.000],  loss: 3.112533, mean_q: 7.208824, mean_eps: 0.100000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 34923/100000: episode: 69, duration: 626.560s, episode steps: 365, steps per second:   1, episode reward: 60.000, mean reward:  0.164 [ 0.000, 25.000], mean action: 2.403 [0.000, 5.000],  loss: 2.026346, mean_q: 7.070735, mean_eps: 0.100000\n",
      " 35784/100000: episode: 70, duration: 1473.617s, episode steps: 861, steps per second:   1, episode reward: 490.000, mean reward:  0.569 [ 0.000, 200.000], mean action: 2.027 [0.000, 5.000],  loss: 3.022921, mean_q: 7.065766, mean_eps: 0.100000\n",
      "done, took 58860.429 seconds\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x248ccc01f30>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dqn.fit(env, nb_steps=100000, visualize=False, verbose=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0216d34",
   "metadata": {},
   "source": [
    "# Testando o modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1b4f18d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing for 10 episodes ...\n",
      "Episode 1: reward: 70.000, steps: 469\n",
      "Episode 2: reward: 55.000, steps: 438\n",
      "Episode 3: reward: 110.000, steps: 497\n",
      "Episode 4: reward: 140.000, steps: 677\n",
      "Episode 5: reward: 30.000, steps: 430\n",
      "Episode 6: reward: 80.000, steps: 303\n",
      "Episode 7: reward: 75.000, steps: 407\n",
      "Episode 8: reward: 145.000, steps: 498\n",
      "Episode 9: reward: 640.000, steps: 1100\n",
      "Episode 10: reward: 150.000, steps: 553\n",
      "149.5\n"
     ]
    }
   ],
   "source": [
    "scores = dqn.test(env, nb_episodes=10, visualize=True)\n",
    "print(np.mean(scores.history['episode_reward']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e7cd3304",
   "metadata": {},
   "outputs": [],
   "source": [
    "dqn.save_weights('dqn_steps_35784_mr_0569.h5f')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e6b92177",
   "metadata": {},
   "outputs": [],
   "source": [
    "del model, dqn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "cdb269f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "dqn.load_weights('dqn_114323.h5f')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
